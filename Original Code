using System; using CADIS.Common; using CADIS.Base; using CADIS.Process; using System.Collections.Generic; using System.Collections.Concurrent; namespace CADIS.Solution.Workflow { 	/// <summary> 	/// Manages the multi-threaded running of a Solution Workflow, including the complex branch/join logic 	/// </summary> 	internal class ThreadManager : IThreadManager 	{ 	// Use a queue to ensure the order of logging is maintained 	// Concurrent because this class must be threadsafe 	ConcurrentQueue<KeyValuePair<Int64, String>> mLog; 	Int32 mCurrentThreads = 0; 	Int32 mMaxThreads; 	Queue<IRunStep> mQueue = new Queue<IRunStep>(); 	Object mQueueLock = new Object(); 	bool mAborted = false; 	System.Threading.ManualResetEvent mAbortEvent; 	Int32 mTopLevelRunID; 	Int32 mParentRunID; 	Int32 mRunID; 	LockingSystem mParentLockMode; 	IWorkflowController mController; 	public ThreadManager(Messaging.RunInfo parentRunInfo, Int32 maxThreads, IWorkflowController controller, IRunStep startStep) 	{ 	mLog = new ConcurrentQueue<KeyValuePair<Int64, String>>(); 	mTopLevelRunID = parentRunInfo.TopLevelRunID; 	mParentRunID = parentRunInfo.ParentRunID; 	mRunID = parentRunInfo.RunID; 	mParentLockMode = parentRunInfo.ParentLockMode; 	mController = controller; 	mMaxThreads = maxThreads; 	mAbortEvent = new System.Threading.ManualResetEvent(false); 	// Add the starting step to the queue ready for it to start when StartNext is called 	QueueThread(startStep); 	} 	/// <summary> 	/// Attempt to start the steps on the queue, up to the maximum number of threads we are permitted to run 	/// </summary> 	public void StartNext() 	{ 	List<Int32> startedKeys = new List<Int32>(); 	lock (mQueueLock) 	{ 	// The ThreadPool should be safe to use as there are typically 1023 threads available, and 	// even with a complex nested solution containing Data Flow components we're unlikely to run out 	int worker, port; 	System.Threading.ThreadPool.GetAvailableThreads(out worker, out port); 	if (worker <= 0) 	{ 	LogMessage("Ran out of threadpool threads"); 	// Quit gracefully 	RequestStop(); 	} 	else 	{ 	// If we have spare threads (in our count) and items left on the queue, start them off 	while (mCurrentThreads < mMaxThreads && mQueue.Count > 0) 	{ 	// Pick next runstep off the top of the queue 	IRunStep rs = mQueue.Dequeue(); 	startedKeys.Add(rs.Key); 	// Increase the counter as we're about to start this thread 	mCurrentThreads++; 	// Start the new thread 	System.Threading.ThreadPool.QueueUserWorkItem(rs.Run); 	} 	} 	LogMessage("********** Starting next steps *********"); 	foreach (Int32 k in startedKeys) 	{ 	LogMessage("TM: Starting " + k + ", threads: " + mCurrentThreads + ", queue: " + mQueue.Count); 	} 	LogMessage("Queue contains keys: "); 	foreach (IRunStep item in mQueue) 	{ 	LogMessage(item.Key.ToString()); 	} 	if (mQueue.Count == 0 && mCurrentThreads == 0) 	{ 	// Notify the UI that we've finished 	NotifyAllComplete(); 	} 	} 	} 	// Abort the run.  Stop any more items from being queued in the future, and clear the existing queue 	public void RequestStop() 	{ 	lock (mQueueLock) 	{ 	// Running steps check this signal 	mAbortEvent.Set(); 	// New steps check this flag 	mAborted = true; 	// Remove any pending steps 	while (mQueue.Count > 0) 	{ 	mQueue.Dequeue(); 	} 	} 	LogMessage("Stop requested, threads: " + mCurrentThreads + ", queue: " + mQueue.Count); 	} 	// Must be called within a lock statement as it manipulates the queue 	void QueueThread(IRunStep rs) 	{ 	// Don't queue if the abort flag has been set.  Just allow the current steps to complete 	if (mAborted) return; 	// Add the specified item to the queue, but don't start it yet 	mQueue.Enqueue(rs); 	LogMessage("Queued " + rs.Title + "(" + rs.Key + "), threads: " + mCurrentThreads + ", queue: " + mQueue.Count); 	} 	// Must be called within a lock statement as it manipulates the queue 	void ReleaseThread(IRunStep rs, bool andStartNext) 	{ 	LogMessage("Releasing thread for " + rs.Title + "(" + rs.Key + ").  Threads: " + mCurrentThreads + ", queue: " + mQueue.Count); 	mCurrentThreads--; 	if (mCurrentThreads < 0) throw new InvalidReleaseException(); 	if (andStartNext) 	{ 	StartNext(); 	} 	} 	void NotifyStepStart(IRunStep rs) 	{ 	LogMessage(rs.Title + "(" + rs.Key + "): Started"); 	mController.Started(rs.Key); 	} 	void NotifyStepEnd(IRunStep rs, RunResultInfo resultInfo) 	{ 	LogMessage(rs.Title + "(" + rs.Key + "): Completed"); 	mController.Completed(rs.Key, resultInfo); 	} 	// All the workflow steps have completed one way or another (they may also have aborted) 	void NotifyAllComplete() 	{ 	LogMessage("Execution stopped."); 	if (mAborted) 	{ 	mController.Stopped(RunResult.Aborted, null); 	} 	else 	{ 	mController.Stopped(RunResult.Success, null); 	} 	} 	// Log a debug message in memory with high-granularity so we can see the order in which the multithreaded events happen 	public void LogMessage(string msg) 	{ 	// Concurrent queue is thread safe, so no lock required 	mLog.Enqueue(new KeyValuePair<Int64, String>(DateTime.Now.Ticks, msg)); 	} 	public ConcurrentQueue<KeyValuePair<Int64, String>> LogMessages 	{ 	get { return mLog; } 	} 	#region IThreadManager - called by RunStepBase classes 	int IThreadManager.TopLevelRunID 	{ 	get { return mTopLevelRunID; } 	} 	int IThreadManager.ParentRunID 	{ 	get { return mParentRunID; } 	} 	int IThreadManager.RunID 	{ 	get { return mRunID; } 	} 	LockingSystem IThreadManager.ParentLockMode 	{ 	get { return mParentLockMode; } 	} 	bool IThreadManager.HasAborted(int millisecondsToWait) 	{ 	return mAbortEvent.WaitOne(millisecondsToWait, false); 	} 	// ======================================================================================= 	// Pre-run functionality: 	// 	// 1) If my step status is not "NotRun" then immediately release the thread that I've been given and quit.  This happens 	//     because often there are multiple entries for a step on the queue, and the first one which can run does so.  Others then 	//     call this again, but don't need to run because the step has done so already. 	// 	// 2) Check all the previous steps have completed before running this step.  This is a basic prerequisite of the work flow. 	//     If not all the steps have run, we quit and way for this step to be rerun at a later point when all have completed. 	// 	// 3) Check that if this step is the end-point of a decision branch that at least one of the branches has run before 	//    continuing. (We ignore any decision branches where the root node has not executed as the skipping will already have 	//    been handled 	// 	// 4) Finally set this step's status to "Running" to indicate that it is in progress (and cannot be modified) before releasing the lock 	// 	// All of this is critical, and cannot be interrupted by another thread.  Hence it is wrapped up in a SyncLock block 	// ======================================================================================= 	bool IThreadManager.StepCanStart(IRunStep rs) 	{ 	lock (mQueueLock) 	{ 	if (rs.RunResult != RunResult.NotRun) 	{ 	ReleaseThread(rs, true); 	LogMessage(rs.Title + "(" + rs.Key.ToString() + "): Quitting because step is already running or has already run."); 	return false; 	} 	foreach (RunStepInfo prs in rs.ImmedPrevSteps) 	{ 	if (prs.RunStep.StepNotComplete) 	{ 	ReleaseThread(rs, true); 	LogMessage(rs.Title + "(" + rs.Key.ToString() + "): Quitting because not all previous steps have completed."); 	return false; 	} 	} 	if (!rs.ShouldRun) 	{ 	((IThreadManager)this).StepCompleted(rs, new RunResultInfo(RunResult.Skipped)); 	LogMessage(rs.Title + "(" + rs.Key.ToString() + "): Quitting because the run expression evaluated to false: " + rs.RunExpression.ToString()); 	return false; 	} 	rs.RunResult = RunResult.Running; 	NotifyStepStart(rs); 	return true; 	} 	} 	// ======================================================================================= 	// Post-run functionality: 	// 	// 1) Queue up all next steps ready to be run.  Each step is responsible for whether it should actually run according 	//     to the state of its previous steps 	// 	// 2) Set my actual run result property to be the runResult specified 	// 	// 3) Remove the thread that I'm running on from the thread count 	// 	// All of this must be atomic, it cannot be interrupted by another thread.  Hence it is wrapped up in a lock block 	// ======================================================================================= 	void IThreadManager.StepCompleted(IRunStep rs, RunResultInfo runResultInfo) 	{ 	lock (mQueueLock) 	{ 	LogMessage("********** Step Completed for " + rs.Title + "(" + rs.Key + ") *********"); 	LogMessage("Result: " + runResultInfo.RunResult.ToString()); 	foreach (RunStepInfo nrs in rs.ImmedNextSteps) 	{ 	// Queue all next steps. The step itself takes responsibility for deciding whether or not it will run. 	QueueThread(nrs.RunStep); 	} 	// Finally set the step's status 	rs.RunResultInfo = runResultInfo; 	// And release the thread that this step is running on.  Don't start another inside the lock, that will be handled below 	ReleaseThread(rs, false); 	} 	// Inform the UI 	NotifyStepEnd(rs, runResultInfo); 	// Now that we're done, kick off the next steps if there are any 	StartNext(); 	} 	void IThreadManager.NotifyFailure(Exception ex) 	{ 	LogMessage("Failure notified, threads: " + mCurrentThreads + ", queue: " + mQueue.Count + Environment.NewLine + Environment.NewLine + ex.Message); 	lock (mQueueLock) 	{ 	while (mQueue.Count > 0) 	{ 	mQueue.Dequeue(); 	} 	} 	mController.Stopped(RunResult.FailureAbort, ex); 	} 	void IThreadManager.NotifyIterationStarted(int key, int iteration) 	{ 	LogMessage(key + ": Iteration " + iteration + " started"); 	mController.IterationStarted(key, iteration); 	} 	void IThreadManager.NotifyIterationCompleted(int key, int iteration, RunResultInfo resultInfo) 	{ 	LogMessage(key + ": Iteration " + iteration + " completed"); 	mController.IterationCompleted(key, iteration, resultInfo); 	} 	// Used to send an overall workflow success/failure status back to the workflow 	void IThreadManager.SetWorkflowRunResult(RunResult result) 	{ 	mController.UpdateWorkflowRunResult(result); 	} 	// Used to send the overall return code integer value back to the workflow 	void IThreadManager.SetWorkflowReturnCode(UserResultCode resultCode) 	{ 	mController.UpdateWorkflowReturnCode(resultCode); 	} 	void IThreadManager.LogMessage(string msg) 	{ 	LogMessage(msg); 	} 	#endregion 	public class InvalidReleaseException : Exception 	{ 	public override string Message 	{ 	get { return "Cannot release more threads than have been allocated"; } 	} 	} 	} } using System; using System.Collections.Generic; using System.Text; namespace CADIS.CPU { 	/// <summary> 	/// Monitors the specified process ID and notifies the user when the process is no longer running. 	/// More reliable and less resource intensive than monitoring the process using events 	/// </summary> 	public class ProcessMonitor 	{ 	public event EventHandler ProcessExited; 	Int32 mPid; 	System.Timers.Timer mTimer; 	Object mLock = new Object(); 	public ProcessMonitor(Int32 pid) 	{ 	mPid = pid; 	mTimer = new System.Timers.Timer(5000); 	mTimer.AutoReset = false; 	mTimer.Elapsed += new System.Timers.ElapsedEventHandler(Timer_Elapsed); 	mTimer.Start(); 	} 	public Int32 ProcessID 	{ 	get 	{ 	lock (mLock) 	{ 	return mPid; 	} 	} 	} 	public Boolean IsRunning 	{ 	get 	{ 	try 	{ 	lock (mLock) 	{ 	System.Diagnostics.Process p = System.Diagnostics.Process.GetProcessById(mPid); 	return true; 	} 	} 	catch (ArgumentException) 	{ 	return false; 	} 	} 	} 	public void KillProcess() 	{ 	try 	{ 	lock (mLock) 	{ 	System.Diagnostics.Process p = System.Diagnostics.Process.GetProcessById(mPid); 	p.Kill(); 	} 	} 	catch (ArgumentException) 	{ 	// Process not found, assumed already ended 	} 	} 	void Timer_Elapsed(object sender, System.Timers.ElapsedEventArgs e) 	{ 	if (this.IsRunning) 	{ 	// Restart the timer 	mTimer.Start(); 	} 	else 	{ 	// Process not found 	OnProcessExited(); 	} 	} 	void OnProcessExited() 	{ 	if (ProcessExited != null) ProcessExited(this, EventArgs.Empty); 	} 	} } using System; using System.Threading; using CADIS.Common; namespace CADIS.Service.Common { 	/// <summary> 	/// Heartbeat class for periodic server processes.   	/// </summary> 	/// <remarks> 	/// Save excessing calls to server for current time (with periodic resyncs)	 	/// Facilitates testing of time period service code 	/// Abstract repeated boilerplate timer locking code away from client code 	/// </remarks> 	public class ServerTimeHeartbeat : IServerTimeHeartbeat 	{ 	private readonly IServerInfo mServerInfo; 	private readonly int mIntervalSecs; 	private readonly int mTicksBetweenResync; 	private readonly bool mTickOnStart; 	private readonly object mTimerThreadLock = new Object(); 	private readonly System.Timers.Timer mPollIntervalTimer; 	private DateTime mServerTime; 	private int mTicksUntilResync; 	private bool mRunning; 	public event TickEventHandler TickEvent; 	public bool Running { get { return mRunning; } } 	public ServerTimeHeartbeat(IServerInfo serverInfo, bool tickOnStart, int intervalSecs, int serverTimeResyncMins = 5) 	{ 	mRunning = false; 	mServerInfo = serverInfo; 	mTickOnStart = tickOnStart; 	mIntervalSecs = intervalSecs; 	mTicksBetweenResync = CalcTicksBetweenResync(serverTimeResyncMins); 	mPollIntervalTimer = CreateTimer();	 	} 	int CalcTicksBetweenResync(int serverTimeResyncMins) 	{ 	float ticksPerResync = (serverTimeResyncMins*60.0f)/(float) mIntervalSecs; 	return (ticksPerResync < 1) ? 1 : (int) Math.Ceiling(ticksPerResync); 	} 	private System.Timers.Timer CreateTimer() 	{ 	var timer = new System.Timers.Timer(); 	timer.Elapsed += new System.Timers.ElapsedEventHandler(mEventPollTimer_Elapsed); 	timer.Enabled = false; 	timer.Interval = mIntervalSecs * 1000; 	timer.AutoReset = true; 	return timer; 	} 	void mEventPollTimer_Elapsed(object sender, System.Timers.ElapsedEventArgs e) 	{ 	if (Monitor.TryEnter(mTimerThreadLock)) 	{ 	try 	{ 	UpdateTime(); 	RaiseTickEvent(); 	} 	finally 	{ 	Monitor.Exit(mTimerThreadLock); 	}	 	} 	} 	private void RaiseTickEvent() 	{ 	if (TickEvent != null) 	{ 	foreach (Delegate receiver in TickEvent.GetInvocationList()) 	{ 	((TickEventHandler)receiver).BeginInvoke(mServerTime, null, null); 	} 	} 	} 	/// <summary> 	/// Time is periodically resynced from server, or incremented by tick period 	/// to avoid unecessary repeated calls to server for a time a known interval ago 	/// </summary> 	private void UpdateTime() 	{ 	if (--mTicksUntilResync == 0) 	{ 	RefreshTimeFromServer(); 	} 	else 	{ 	mServerTime = mServerTime.AddSeconds(mIntervalSecs); 	} 	} 	/// <summary> 	/// Resync clock, and restart tick countdown until next sync 	/// </summary> 	private void RefreshTimeFromServer() 	{ 	mTicksUntilResync = mTicksBetweenResync; 	mServerTime = TruncateMilliseconds(SqlHelper.Service.Process.GetServerTime(mServerInfo.NewCallInfo())); 	} 	/// <summary> 	/// Lowest resolution is 1 second 	/// </summary> 	private static DateTime TruncateMilliseconds(DateTime dateTime) 	{ 	return dateTime.AddTicks(-(dateTime.Ticks % TimeSpan.TicksPerSecond)); 	} 	public void Start() 	{ 	lock (mTimerThreadLock) 	{ 	mPollIntervalTimer.Stop(); 	RefreshTimeFromServer(); 	if (mTickOnStart) RaiseTickEvent(); 	mPollIntervalTimer.Start(); 	mRunning = true; 	} 	} 	public void Stop() 	{ 	lock (mTimerThreadLock) 	{ 	mPollIntervalTimer.Stop(); 	mRunning = false; 	} 	} 	~ServerTimeHeartbeat() 	{ 	try 	{ 	this.Stop(); 	TickEvent = null; 	} 	catch { } 	} 	} } using System; using System.Diagnostics; using System.Threading; using CADIS.Common; using CADIS.Service.Common; namespace CADIS.Service { 	/// <summary> 	/// Adapter class to host polling process instance in a service instance 	/// </summary> 	public class PollingInstance : IProcessInstance 	{ 	private readonly object mHeartbeatThreadLock = new Object(); 	private readonly IServerTimeHeartbeat mServerTimeHeartbeat; 	private readonly IProcessInstancePolling mProcessInstancePolling; 	private readonly IProcessComponent mProcess; 	private readonly IServiceLogger mServiceLogger; 	public PollingInstance(IProcessComponent processComponent, IProcessInstancePolling processInstancePolling, IServiceLogger serviceLogger, IServerInfo serverInfo) 	: this(processComponent, processInstancePolling, serviceLogger, new ServerTimeHeartbeat(serverInfo, false, processComponent.PollingInterval)) { } 	// Allow DI of heartbeat for testing 	public PollingInstance(IProcessComponent processComponent, IProcessInstancePolling processInstancePolling, IServiceLogger serviceLogger, IServerTimeHeartbeat serverTimeHeartbeat) 	{ 	mProcessInstancePolling = processInstancePolling; 	mProcess = processComponent; 	mServerTimeHeartbeat = serverTimeHeartbeat; 	mServiceLogger = serviceLogger; 	} 	private void ServerTimeHeartbeat_Tick(DateTime serverTime) 	{ 	// If tick frequency set such that previous tick still running, just 	// fall though until next call to avoid unnecessary deadlocking 	if (Monitor.TryEnter(mHeartbeatThreadLock)) 	{ 	try 	{ 	LogMessage($"Poll instance heartbeat start. ", $"Server Time : {serverTime}", ServiceLogLevel.Debug); 	var stopwatch = Stopwatch.StartNew(); 	PrePoll(serverTime); 	stopwatch.Stop(); 	LogMessage($"Poll instance heartbeat complete. ", $"Duration : {stopwatch.Elapsed.TotalSeconds } s", ServiceLogLevel.Debug); 	} 	catch (Exception ex) 	{ 	// No exceptions should ever be thrown out of this method as they will crash the service! 	LogException(ex); 	} 	finally 	{ 	Monitor.Exit(mHeartbeatThreadLock); 	} 	} 	else 	{ 	LogMessage("Service heartbeat skipped. ", "Previous processing cycle still executing", ServiceLogLevel.Debug); 	} 	} 	/// <summary> 	/// Fired when the event time elapses.  Runs on a background thread 	/// This is used to check whether the process definition has changed and whether it needs to be reloaded 	/// </summary> 	private void PrePoll(DateTime serverTime) 	{ 	// Quit if the process is not active (i.e. either disabled or not within watch period) 	if (!mProcess.IsActive(serverTime)) return; 	// Poll the watch instance 	LogMessage("Polling the process.", "", ServiceLogLevel.Debug); 	mProcessInstancePolling.Poll(); 	} 	public System.Runtime.Serialization.ISerializable GetState() 	{ 	LogMessage("Getting current process state.", "", ServiceLogLevel.Debug); 	return mProcessInstancePolling.GetState(); 	} 	public void RestoreState(System.Runtime.Serialization.ISerializable state) 	{ 	LogMessage("Restoring process state.", "", ServiceLogLevel.Debug); 	mProcessInstancePolling.RestoreState(state); 	} 	public void StartProcess() 	{ 	// In future the polling instance will host an IMessageTarget and there will be a : 	// mMessageTarget.Initialise(); 	// call to it here that will perform the failover prior to polling starting 	// (via the ProcessRunFailover domain service who's logic should be push/poll independent) 	LogMessage("Starting polling process.", "", ServiceLogLevel.Debug); 	mProcessInstancePolling.Initialise(); 	StartHeartbeat(); 	} 	public void StopProcess() 	{ 	LogMessage("Stopping polling process.", "", ServiceLogLevel.Debug); 	StopHeartbeat(); 	mProcessInstancePolling.Terminate(); 	} 	private void StartHeartbeat() 	{ 	LogMessage("Starting poll instance heartbeat.", "", ServiceLogLevel.Debug); 	if (mServerTimeHeartbeat.Running) StopHeartbeat(); 	mServerTimeHeartbeat.TickEvent += new TickEventHandler(ServerTimeHeartbeat_Tick); 	mServerTimeHeartbeat.Start(); 	} 	private void StopHeartbeat() 	{ 	LogMessage("Stopping poll instance heartbeat.", "", ServiceLogLevel.Debug); 	mServerTimeHeartbeat.Stop(); 	mServerTimeHeartbeat.TickEvent -= new TickEventHandler(ServerTimeHeartbeat_Tick); 	} 	private void LogMessage(string summary, string detail, ServiceLogLevel logLevel) 	{ 	mServiceLogger.LogMessage(mProcess.ComponentKey, mProcess.Guid, mProcess.Name, summary, detail, logLevel); 	} 	public void LogException(Exception ex) 	{ 	mServiceLogger.LogException(mProcess.ComponentKey, mProcess.Guid, mProcess.Name, ex); 	} 	} } using System; using System.Collections.Generic; using System.Text; using System.Collections.Concurrent; using System.Diagnostics; using System.Threading; using CADIS.DataFlow.Runtime.Core; using CADIS.DataFlow.Runtime.Data; using System.Globalization; using CADIS.Common; namespace CADIS.DataFlow.Runtime.Distribution { 	/// <summary> 	/// The head of a number of parallel processing chains. 	/// This class manages the distribution of a single set of data to a number of threads at high speed.  It uses 	/// buffers to load up each thread with work to do.  It is only efficient with multiple threads. 	/// 	/// It tries to be fair by ensuring each thread gets a similar amount of data to start with, and then responds 	/// to each thread's request for more data as required. 	/// Critically, each thread waits to be given more data.  Therefore this class is not swamped with threads 	/// requesting data directly, they are queued and served more data when the Distributor is ready. 	/// 	/// NOTE: This class uses advanced multi-threading techniques and should not be modified except with EXTREME care! 	/// </summary> 	public class DataDistributor : HeadBase, IDistributor 	{ 	const double WAIT_WARNING_THRESHOLD = 35.0; 	Stopwatch mElapsedTimer; 	Stopwatch mWaitingTimer; 	// An auto-reset because we use it multiple times and just want a pulse notification 	AutoResetEvent mWaitForEmptyThreads; 	// A manual-reset because this is only signalled once 	ManualResetEvent mWaitForAllThreadsToComplete; 	const String CONTEXT = "Distributor"; 	List<DistributedThread> mAllThreads; 	// I choose to use a queue so that each thread gets a fair share of the workload.  The downside is that 	// if too many threads are created, each doesn't do a lot of work. 	ConcurrentQueue<DistributedThread> mEmptyThreads; 	Int64 mThreadsRunning; 	Int64 mFailedThreads; 	IBufferReader mReader; 	Boolean mRunOnce; 	public DataDistributor(IBufferReader reader, ThreadLog log, CultureInfo cultureToRunAs, Int32 runID, Int32 parentRunId, Int32 topLevelRunId, ICallInfo ci) 	: base(log, cultureToRunAs, runID,parentRunId,topLevelRunId, ci) 	{ 	mReader = reader; 	mAllThreads = new List<DistributedThread>(); 	mEmptyThreads = new ConcurrentQueue<DistributedThread>(); 	// Set to true initially as we don't want to wait the first time in (all threads are empty) 	mWaitForEmptyThreads = new AutoResetEvent(true); 	mWaitForAllThreadsToComplete = new ManualResetEvent(false); 	mElapsedTimer = new Stopwatch(); 	mWaitingTimer = new Stopwatch(); 	// The initial state of signals above is vital.  Therefore we force the case that we can only run this process 	// once without throwing an exception.  	// This is a preemptive safety measure to avoid some nasty thread timing issues which were seen 	// when debugging this code. 	mRunOnce = false; 	} 	public void AddThread(Int32 id, Int32 bufferSize, IDistributedTarget target) 	{ 	// Wrap the distribution target in our high-performance buffering class 	DistributedThread thread = new DistributedThread(this.Log, this, id, bufferSize, target, mCultureToRunAs); 	mAllThreads.Add(thread); 	// Initially, all threads are empty 	mEmptyThreads.Enqueue(thread); 	} 	protected override void RunProcess() 	{ 	// Initialisation of the signals CAN ONLY be done in the constructor, before the process starts running, to 	// avoid race conditions.  Use a flag just to make sure that a Dev doesn't change this in the future by accident 	if (mRunOnce) throw new ApplicationException("The distributor can only be run once.  Recreate the object to ensure it is initialised correctly, don't run it twice"); 	mRunOnce = true; 	// Set the total number of threads so we can monitor when all have completed 	Interlocked.Exchange(ref mThreadsRunning, mAllThreads.Count); 	Interlocked.Exchange(ref mFailedThreads, 0); 	// Start a thread for each of the distributed workers.  They will enter their wait state 	// until we feed them their first set of data 	foreach (DistributedThread thread in mAllThreads) 	{ 	ThreadPool.QueueUserWorkItem(thread.Run); 	} 	// Loop forever servicing empty threads, until there's no more data and all threads have completed 	Boolean endOfData = false; 	mElapsedTimer.Start(); 	while (true) 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Waiting for empty or completed threads"); 	// Wait until either all threads have completed or new empty threads have been added to the queue 	mWaitingTimer.Start(); 	Int32 waitFlagged = WaitHandle.WaitAny(new WaitHandle[] { mWaitForAllThreadsToComplete, mWaitForEmptyThreads }); 	mWaitingTimer.Stop(); 	// If the WaitForAllThreadsComplete signal was set, quit the loop 	if (waitFlagged == 0) 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Received \"all threads completed\" signal"); 	mElapsedTimer.Stop(); 	int waitpercent = mElapsedTimer.ElapsedMilliseconds == 0 ? 0 : (int)((100.0d * mWaitingTimer.ElapsedMilliseconds) / mElapsedTimer.ElapsedMilliseconds); 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Distributor was waiting for threads to complete {0}% of the time.", waitpercent); 	this.Log.LogMessage(ThreadLog.LogLevel.INFO, CONTEXT, "Distributor was waiting for threads to complete {0}% of the time.", waitpercent); 	break; 	} 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Received empty threads signal.  Threads in empty buffer queue: {0}", mEmptyThreads.Count); 	// Load all the empty threads with data whilst there is data available 	while (!mEmptyThreads.IsEmpty) 	{ 	// Remove the first item from the empty threads list and refill it 	DistributedThread thread; 	mEmptyThreads.TryDequeue(out thread); 	// Check whether any other threads have failed.  If so, stop good threads once their buffers are empty 	if (Interlocked.Read(ref mFailedThreads) > 0) 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Refill of thread {0} aborted because another thread has failed", thread.ID); 	// Allow to fall into the end of data processing, which stops the threads naturally 	endOfData = true; 	} 	if (endOfData) 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Signalling thread {0} \"no more data\"", thread.ID); 	// We are at the end of the data source, so we can't refill the thread.  Indicate to it that 	// it should now stop when it's ready 	thread.SignalNoMoreData(); 	} 	else 	{ 	// Read a set of lines from the data source 	Int32 rowCount = thread.RefillQueue(mReader); 	// Set the flag if we've hit the end of the data 	endOfData = mReader.AtEnd; 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Refill of thread {0} complete with {1} rows.  At end of data: {2}", thread.ID, rowCount, endOfData); 	} 	} 	} 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Clearing up"); 	// Capture any exceptions thrown by the worker threads 	foreach (DistributedThread thread in mAllThreads) 	{ 	if (thread.LastException != null) this.Exceptions.Add(thread.LastException); 	} 	} 	void IDistributor.NotifyQueueEmpty(DistributedThread thread) 	{ 	// NOTE: Runs on the DistributedThread background thread! 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Thread {0} notified its buffer is empty", thread.ID); 	mEmptyThreads.Enqueue(thread); 	// Tell the main loop that there's work to be done 	mWaitForEmptyThreads.Set(); 	} 	void IDistributor.NotifyComplete(DistributedThread thread, Boolean failed) 	{ 	// NOTE: Runs on the DistributedThread background thread! 	if (failed) 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Thread {0} notified it failed", thread.ID); 	// Indicate we should stop at the next appropriate opportunity 	Interlocked.Increment(ref mFailedThreads); 	} 	else 	{ 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "Thread {0} notified it is complete", thread.ID); 	} 	// Decrement the counter in a thread-safe way 	Int64 remaining = Interlocked.Decrement(ref mThreadsRunning); 	if (remaining <= 0) 	{ 	// Signal to the main loop that we're done 	this.Log.LogMessage(ThreadLog.LogLevel.DEBUG, CONTEXT, "All threads are complete"); 	mWaitForAllThreadsToComplete.Set(); 	} 	} 	protected override void Close(ConcurrentDictionary<int, RunTimer> metrics) 	{ 	try 	{ 	foreach (DistributedThread thread in mAllThreads) 	{ 	thread.Close(metrics); 	} 	} 	finally 	{ 	mReader.Close(metrics); 	} 	} 	public override long TotalTransactions 	{ 	get { return mReader.TotalTransactions; } 	} 	} }
